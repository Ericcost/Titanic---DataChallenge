{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cb7c25-8e6c-4867-9b35-9592d7c3eca6",
   "metadata": {},
   "source": [
    "# [DataChallenge Titanic](https://www.kaggle.com/competitions/titanic/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c81b10-7379-405c-951b-3ee27d286811",
   "metadata": {},
   "source": [
    "Dans le cadre du [DataChallenge Titanic](https://www.kaggle.com/competitions/titanic/overview)  proposé dans le cours de Machine Learning chez YNOV, ce document servira de résumé descriptif pour [chacune des itérations soumises](https://www.kaggle.com/competitions/titanic/submissions) sur le site de Kaggle. \n",
    "\n",
    "Chaque itération décrite ci-après comprendra au moins un titre, une description, un algorithme de machine learning et le résultat associé. Dans certains cas on précisera quelles ont été les stratégies en détaillant par exemple la méthode utilisé pour l'optimisation, du code, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aff987-04de-4795-b321-5dd4959169d8",
   "metadata": {},
   "source": [
    "# Première Itération : \n",
    "## Titre : Un petit tour sur une proposition formulée par ChatGPT\n",
    "\n",
    "### Algorithme :\n",
    "RandomForest \n",
    "\n",
    "### Description : \n",
    "Pour un premier jet, je voulais simplement tester la 'puissance' de ChatGPT, je lui ai donc soumis un prompt de manière à obtenir un premier résultat et avoir un aperçu du score que pouvait faire chatgpt en lui fournissant des consignes primaires :\n",
    "\n",
    "```\n",
    "I have a titanic dataset with PassengerID Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked and survived\n",
    "I would like to make supervised learning by using a Model to train my algorithm. The target variable is  Survived others are caractértistiques variable how can i proceed to make a first simple submission\n",
    "```\n",
    "\n",
    "J'ai dû lui préciser certaines problématiques rencontrées comme les valeurs manquantes présentes dans certaines colonnes par exemple mais globalement je suis allé à l'essentiel pour obtenir de quoi soumettre une première version signé chatGPT.\n",
    "\n",
    "### Code : \n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Handle missing values\n",
    "train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].median(), inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)\n",
    "train_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace=True)\n",
    "test_data['Embarked'].fillna(test_data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical variables into numeric\n",
    "train_data['Sex'] = train_data['Sex'].map({'male': 0, 'female': 1})\n",
    "test_data['Sex'] = test_data['Sex'].map({'male': 0, 'female': 1})\n",
    "train_data['Embarked'] = train_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "test_data['Embarked'] = test_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "\n",
    "# Verify the conversion\n",
    "print(train_data[['Sex', 'Embarked']].head())\n",
    "print(test_data[['Sex', 'Embarked']].head())\n",
    "\n",
    "# Select features\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['Survived']\n",
    "X_test = test_data[features]\n",
    "\n",
    "# Create the model\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Prepare submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_data['PassengerId'],\n",
    "    'Survived': predictions\n",
    "})\n",
    "\n",
    "# Save the submission to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "```\n",
    "\n",
    "### Score :\n",
    "0.77751\n",
    "\n",
    "### Conclusion : \n",
    "On va chercher à explorer diverses pistes en commençant par le choix des algorithmes et l'objectif sera dans un premier temps de faire une proposition personnel d'un model, dans un second temps d'optimiser mes recherches et donc 'in fine' le résultat du model pour battre ChatGPT et enfin chercher à se rapprocher le plus possible du meilleur score, soit 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f503f34-a484-4517-9247-7abb1217118c",
   "metadata": {},
   "source": [
    "# Deuxième Itération : \n",
    "## Titre : On démarre par la regression avec DecisionTreeRegressor\n",
    "\n",
    "### Algorithme :\n",
    "Pour cette première itération, nous choisissons d'utiliser l'arbre de décision. \n",
    "\n",
    "### Description :\n",
    "Aborder un sujet de ML pour la première fois est un challenge. Les stratégies à déployer sont multiples et avant même l'écriture d'une seule ligne de code, le choix d'un algorithme plus ou moins complexe, les méthodes d'optimisation, etc.. on peut déjà se sentir dépassé par l'immensité des choix à faire à chaque itération. Pour éviter de se perdre au carrefour des chemins que l'on peut emprunter, nous choisissons de faire simple. Ainsi, nous allons démarrer par ce qu'on a le plus aborder en cours : La Regression. \n",
    "\n",
    "En s'inspirant d'un jupiter notebook vu en cours, nous avons adopter une sratégie très conservatrice, c'est-à-dire, une stratégie qui fait perdre beaucoup de données ici on a supprimer toutes les colonnes contenant des valeurs manquantes (Age, Cabin, Embarked) et celles n'étant pas des valeurs numériques (Name, Sex, Ticket). \n",
    "\n",
    "Nous avons ensuite séparer nos variables caractéristiques de notre variable cible pour entrainer notre model d'arbre de décision sur notre jeu de données: \n",
    "\n",
    "```\n",
    "titanic_features = ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fare']\n",
    "X = df[titanic_features]\n",
    "y = df.Survived\n",
    "```\n",
    "\n",
    "Puis nous allons séparer nos données d'entrainements pour obtenir un jeu d'entrainement et un jeu de validation. Ensuite, nous allons entrainer notre model `DecisionTreeRegressor` et procéder à une validation.\n",
    "\n",
    "```\n",
    "## Validation \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8) ## Possibilité de split le dataset différemment selon un paramètre à chercher pour les prochaines itérations\n",
    "\n",
    "# instanciation du modèle\n",
    "titanic_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# entraînement (fit)\n",
    "titanic_model.fit(X_train, y_train)\n",
    "\n",
    "# obtenir un score global (par défaut la metrique obtenu correspond à l'accuracy)\n",
    "titanic_model.score(X_test, y_test)\n",
    "\n",
    "## Le score obtenu est négatif, il devrait être compris entre 0 et 1. La méthode `.score` renvoie un coefficient de détermination R2 qui mesure la proportion de la variance des données qui est expliquée par le modèle. Un R2 de 1 indique un modèle parfait, tandis qu'un R2 de 0 indique que le modèle ne fait pas mieux que la moyenne des données.\n",
    "\n",
    "Comment traduire ce résulat ? -0.4464646464646467\n",
    "```\n",
    "\n",
    "Nous décidons de poursuivre jusqu'à la soumission de notre première itération personnel quand bien même nous ne savons expliquer ce score négatif. \n",
    "\n",
    "### Score : \n",
    "Dans un premier temps un score de **0.0000** est obtenu tout simplement car la colonne `Survived` avait été laissé en float, une fois transformé en int64 nous obtenons **0.65071**\n",
    "\n",
    "### Conclusion : \n",
    "Nous avons adopté ici une stratégie hyperconservatrice nous allons maintenant tenter d'améliorer notre résultat en travaillant sur l'encodage de certaines features qui ont été écartées comme par exemple les valeurs de la colonne `Sex`. Nous tenterons également d'optimiser nos paramètrès d'entrer de l'algorithme. À suivre ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21d355-c5c1-406d-9693-95e7624567ff",
   "metadata": {},
   "source": [
    "# Troisième Itération :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c0088-5a60-45bc-9c45-01d822b6f09a",
   "metadata": {},
   "source": [
    "## Titre : On poursuit la regression avec DecisionTreeRegressor(V2)\n",
    "\n",
    "### Algorithme :\n",
    "Pour cette nouvelle itération, nous choisissons de procéder à une deuxième version l'arbre de décision. \n",
    "\n",
    "### Description :\n",
    "À partir de maintenant et ceci pour les futures itérations nous allons chercher à améliorer le précédent score réalisé (0.65071). Les leviers à actionner en ce sens sont les suivants :\n",
    "\n",
    "- Nettoyage des données : Nous allons voir quelles stratégies nous pouvons adopter pour combler les valeurs manquantes (Imputation).\n",
    "- Nous allons faire aussi du feature engineering : C'est-à-dire, réfléchir à comment pourrait-on exploiter les colones de notre datasets original pour extraire des informations différentes et peut-être plus fines permettant d'optimiser `in fine` notre score.\n",
    "- Méthode d'optimisation : Nous allons mettre en place à minima un Grid Search pour tenter d'optimiser nos résultats.\n",
    "- Utilisation d'autres modèles peut-être plus appropriés. \n",
    "\n",
    "Ici nous allons réfléchir dans un premier temps à remplacer les valeurs manquantes des colonnes ` Age`, `Embarked`, `Cabin`.\n",
    "\n",
    "#### Valeurs Manquantes\n",
    "\n",
    "##### Colonne `Age`\n",
    "\n",
    "Nous avons remarqué qu'en mettant en perspective la colonne `Name` avec la colonne `Age`, qui contient 177 valeurs manquantes, on remarque qu'il y a des 'Titres' tels quel 'Mr', 'Mrs', 'Dr', 'Master', 'Miss', etc.. qui nous donne quelques indications sur l'âge que pourrait avoir nos personnes n'ayant pas leurs âges renseignés. Nous allons récupérer uniquement les lignes ayant des valeurs manquantes das la colonnes `Age` et voir comment remplacer de façon pertinente ces valeurs.\n",
    "\n",
    "```\n",
    "# Define a function to extract titles\n",
    "def extract_title(name):\n",
    "    return name.split(',')[1].split('.')[0].strip()\n",
    "\n",
    "# Apply the function to create a 'Title' column\n",
    "train_data['Title'] = train_data['Name'].apply(extract_title)\n",
    "\n",
    "# Display\n",
    "display(HTML(train_data['Title'].to_frame().to_html()))\n",
    "```\n",
    "\n",
    "```\n",
    "# Count occurrences of each title within the train dataset\n",
    "title_counts = train_data['Title'].value_counts()\n",
    "\n",
    "print(title_counts)\n",
    "```\n",
    "\n",
    "Nous décidons alors de créer une colonne `Title`. De cette nouvelle colonne nous allons pouvoir déterminer la valeur median des âges selon les Titres propre à notre datasets. Nous ferons ensuite correspondre cette valeur médiane par Titre à chaque personne du même Titre n'ayant pas encore d'âge renseigné. \n",
    "\n",
    "```\n",
    "selected_titles = ['Mr', 'Miss', 'Mrs', 'Master', 'Dr']\n",
    "filtered_df = train_data[train_data['Title'].isin(selected_titles)]\n",
    "\n",
    "# Calculate median age for each title\n",
    "median_ages = filtered_df.groupby('Title')['Age'].median()\n",
    "\n",
    "# Display median_ages\n",
    "median_ages\n",
    "\n",
    "# Replace NaN values in nan_age_rows['Age'] with median values based on Title\n",
    "for index, row in nan_age_rows.iterrows():\n",
    "    title = row['Title']\n",
    "    median_age = median_ages[title]\n",
    "    nan_age_rows.loc[index, 'Age'] = median_age\n",
    "    train_data.loc[index, 'Age'] = median_age\n",
    "```\n",
    "\n",
    "##### Colonne `Embarked`\n",
    "\n",
    "La majorité des personnes ont embarqués à S (Southampton), soit 684 personnes sur 891. Nous choisissons de remplacer les valeurs manquantes des deux lignes concernées par la valeur S.\n",
    "\n",
    "```\n",
    "\n",
    "# Replace NaN values in 'Embarked' with 'S'\n",
    "train_data['Embarked'] = train_data['Embarked'].fillna('S')\n",
    "\n",
    "```\n",
    "\n",
    "##### Colonne `Cabin`\n",
    "\n",
    "En ce qui concerne la colonne Cabin on remarque qu'il y a 77% environ de valeurs manquantes dans cette colonne. Analysons de plus prêt ces lignes.\n",
    "\n",
    "En cherchant un plan du bateau du titanic on s'aperçoit que le paquebot était constitué de 10 ponts dont 7 avec cabines, allant du pont supérieure A au pont inférieur G. En regardant le plan et en comparant les résulats obtenus plus haut on s'aperçoit que la première lettre des cabines semblent correspondre avec les différents ponts présent sur le bateau.\n",
    "\n",
    "Selon des informations récupérer sur la page wikipédia les [Passagers du Titanic](https://fr.wikipedia.org/wiki/Passagers_du_Titanic) \"la troisième classe est destinée aux nombreux immigrants désireux de s'installer définitivement aux États-Unis.\". Cet article nous informe également que \"Le traitement de ces passagers varie selon la classe. Ainsi, les passagers de troisième classe sont soumis à de stricts contrôles sanitaires lors de l'embarquement, et sont totalement isolés des autres passagers, afin de faciliter les procédures d'arrivée à Ellis Island.\" \n",
    "\n",
    "**Par conséquent on peut imaginer que les ponts E, F et G sont ceux destinées aux personnes de la troisième classe**\n",
    "\n",
    "**Que les ponts D, E et F sont ceux destinées aux personnes de la deuxième classe**\n",
    "\n",
    "**Que les ponts A, B, C et D sont ceux destinées aux personnes de la premère classe**\n",
    "\n",
    "En nous basant la colonne Pclass nous pouvons créer une colonne `Pont_de_Cabin`. Il y aurait à creuser plus en profondeur sur une répartition plus proche de la réalité mais nous choisissons pour cette itération de distribuer de façon aléatoire les potentielles ponts de cabine aux personnes selon leurs classe de la manière suivante : \n",
    "\n",
    "```\n",
    "# Filter rows where 'Pclass' is equal to 1\n",
    "pclass_1_rows = train_data[train_data['Pclass'] == 1]\n",
    "#pclass_1_rows\n",
    "\n",
    "# Further filter to include only rows where 'Cabin' is not NaN\n",
    "pclass_1_rows_with_cabin = pclass_1_rows[pclass_1_rows['Cabin'].notna()]\n",
    "pclass_1_rows_with_cabin\n",
    "\n",
    "# Generate random choices for 'A', 'B', 'C' or 'D'\n",
    "random_choices_class_1 = np.random.choice(['A', 'B', 'C', 'D'], size=len(pclass_1_rows))\n",
    "\n",
    "# Assign these random choices to 'Pont_de_Cabin' for Pclass 2\n",
    "train_data.loc[train_data['Pclass'] == 1, 'Pont_de_Cabin'] = random_choices_class_1\n",
    "```\n",
    "\n",
    "Nous nous sommes également rendu compte que selon les chiffres `paires` et `impaires` que l'on retrouve dans les cabines nous pouvions déterminer de quel côté du bateau se trouvaient les passagers : à babord pour les chiffres pairs à tribord pour les chiffres impairs. \n",
    "\n",
    "```\n",
    "# Function to determine the side based on the cabin number\n",
    "def determine_side(cabin):\n",
    "    if pd.isna(cabin):\n",
    "        return np.nan\n",
    "    else:\n",
    "        # Extract the numerical part of the cabin\n",
    "        num_part = ''.join(filter(str.isdigit, cabin))\n",
    "        if num_part:  # Check if there is a numerical part\n",
    "            num = int(num_part)\n",
    "            if num % 2 == 0:\n",
    "                return 'Babord'\n",
    "            else:\n",
    "                return 'Tribord'\n",
    "        else:\n",
    "            return np.nan\n",
    "```\n",
    "\n",
    "On extrait l'information selon les chiffres présent dans la colonnes cabin\n",
    "\n",
    "```\n",
    "# Apply the function to create the 'Side' column\n",
    "train_data['Side'] = train_data['Cabin'].apply(determine_side)\n",
    "\n",
    "display(HTML(train_data['Side'].to_frame().to_html()))\n",
    "```\n",
    "\n",
    "Le problème c'est que nous ne savons pas pour le moment comment déterminer la répartition des chiffres pairs/impairs selon les persones de manière réaliste.\n",
    "Nous décidons de ne pas aller plus loin dans le remplacement de valeurs manquantes pour cette itération. \n",
    "\n",
    "#### Encodage\n",
    "\n",
    "Dans le but d'étoffer les variables caractéristiques sur lesquelles nous allons entraîner notre modèle nous décidons d'éncoder des variables nous utilisées jusqu'alors. Dans cette itération seule la colonne `Sex` est concernée.\n",
    "\n",
    "##### Colonne `Sex`\n",
    "\n",
    "```\n",
    "# Encode 'Sex' column\n",
    "train_data['Sex'] = train_data['Sex'].map({'male': 0, 'female': 1})\n",
    "```\n",
    "\n",
    "```\n",
    "display(HTML(train_data['Sex'].to_frame().to_html()))\n",
    "```\n",
    "#### Validation \n",
    "\n",
    "Nous allons procéder à la réalisation de notre validation :\n",
    "\n",
    "```\n",
    "titanic_features = ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Sex', 'Age']\n",
    "X = train_data[titanic_features]\n",
    "y = train_data.Survived\n",
    "```\n",
    "\n",
    "##### La méthode Hold-out\n",
    "\n",
    "```\n",
    "## Validation \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8) ## Possibilité de split le dataset différemment selon un paramètre à chercher pour les prochaines itérations\n",
    "\n",
    "# instanciation du modèle\n",
    "titanic_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# entraînement (fit)\n",
    "titanic_model.fit(X_train, y_train)\n",
    "\n",
    "# obtenir un score global (par défaut la metrique obtenu correspond à l'accuracy)\n",
    "titanic_model.score(X_test, y_test)\n",
    "\n",
    "## Le score obtenu est négatif, il devrait être compris entre 0 et 1. La méthode `.score` renvoie un coefficient de détermination R2 qui mesure la proportion de la variance des données qui est expliquée par le modèle. Un R2 de 1 indique un modèle parfait, tandis qu'un R2 de 0 indique que le modèle ne fait pas mieux que la moyenne des données.\n",
    "\n",
    "Comment traduire ce résulat ? -0.12671045429666106\n",
    "```\n",
    "\n",
    "Nous décidons de poursuivre jusqu'à la soumission de notre nouvelle itération personnel quand bien même nous ne savons expliquer ce score négatif. \n",
    "\n",
    "### Score : \n",
    "Nous obtenons un score de **0.74162**\n",
    "\n",
    "### Conclusion : \n",
    "Nous avons ajouté simplement deux features à nos variables caractéristiques. Tout d'abord `Age` qui peut être exploiter maintenant qu'elle ne contient plus de valeur manquantes. Puis `Sex`qui a été encodé. Nous gagnons presque 0.1 point par rapport à notre précédente soumission. Il il y a d'autres voies encore à explorer que nous allons vous partagez dans notre prochaine itération."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569035a0-2552-4ff5-97ef-d96502b3fff7",
   "metadata": {},
   "source": [
    "# Quatrième Itération"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f86cf-e246-405b-8e3d-aaeda3f4e85f",
   "metadata": {},
   "source": [
    "## Titre : On poursuit la regression avec DecisionTreeRegressor(V3)\n",
    "\n",
    "### Algorithme :\n",
    "Pour cette nouvelle itération, nous choisissons de procéder à une troisième version l'arbre de décision. \n",
    "\n",
    "### Description :\n",
    "À partir de maintenant et ceci pour les futures itérations nous allons chercher à améliorer le précédent score réalisé (0.74162). Les leviers à actionner en ce sens sont les suivants :\n",
    "\n",
    "- Nettoyage des données : Nous allons voir quelle(s) stratégie(s) nous pouvons adopter pour combler les valeurs manquantes (Imputation).\n",
    "- Encodage : Nous allons encoder les colonnes non exploitées jusqu'ici pour les ajouterparmi nos variables caractéristiques.\n",
    "- Nous allons faire aussi du feature engineering : C'est-à-dire, réfléchir à comment pourrait-on exploiter les colonnes de notre dataset originel pour extraire des informations différentes et peut-être plus fines permettant d'optimiser `in fine` notre score.\n",
    "- Méthode d'optimisation : Nous allons mettre en place à minima un Grid Search pour tenter d'optimiser nos résultats.\n",
    "- Utilisation d'autres modèles peut-être plus appropriés. \n",
    "\n",
    "Ici nous allons procéder à l'encodage de nos variables `Title`, `Embarked`, `Pont_de_Cabin`, `Side`.\n",
    "\n",
    "#### Encodage\n",
    "\n",
    "##### Colonne `Title`\n",
    "\n",
    "```\n",
    "codes, uniques = pd.factorize(train_data['Title'])\n",
    "\n",
    "train_data['Title_dummies'] = codes\n",
    "\n",
    "train_data[['Title', 'Title_dummies']]\n",
    "```\n",
    "\n",
    "##### Colonne `Embarked`\n",
    "\n",
    "```\n",
    "codes, uniques = pd.factorize(train_data['Embarked'])\n",
    "train_data['Embarked_dummies'] = codes\n",
    "train_data[['Embarked', 'Embarked_dummies']]\n",
    "```\n",
    "\n",
    "##### Colonne `Pont_de_Cabin`\n",
    "\n",
    "```\n",
    "codes, uniques = pd.factorize(train_data['Pont_de_Cabin'])\n",
    "train_data['Pont_de_Cabin_dummies'] = codes\n",
    "train_data[['Pont_de_Cabin', 'Pont_de_Cabin_dummies']]\n",
    "```\n",
    "\n",
    "##### Colonne `Side`\n",
    "\n",
    "```\n",
    "codes, uniques = pd.factorize(train_data['Side'])\n",
    "train_data['Side_dummies'] = codes\n",
    "train_data[['Side', 'Side_dummies']]\n",
    "```\n",
    "\n",
    "Nous sommes conscients que dans ce contexte, les valeurs `NaN` présentes dans la colonne `Side`e sont également encoder et viennent donc créer un certain biais. Il faudra revenir sur ce point dans les futurs itération pour affiner le nettoyage en amont et l'encodage par la suite. Les conséquences de cette encodage sont la création de 4 nouvelles features `Title_dummies`, `Embarked_dummies`, `Pont_de_Cabin_dummies` et `Side_dummies`\n",
    "\n",
    "\n",
    "\n",
    "#### Matrice de correlation\n",
    "\n",
    "Avec un nombre croissant de variables caractéristiques il peut être intéressant de mettre en place une matrice de corrélation pour vérifier l'existence ou non de corrélation entre nos features.\n",
    "\n",
    "```\n",
    "# Compute the covariance matrix\n",
    "covMatrix = np.cov(train_data[['Embarked_dummies', 'Title_dummies', 'Pclass', 'Pont_de_Cabin_dummies']].T, bias=False)\n",
    "\n",
    "# Create a DataFrame for the covariance matrix to add labels\n",
    "cov_df = pd.DataFrame(covMatrix, index=['Embarked_dummies', 'Title_dummies', 'Pclass', 'Pont_de_Cabin_dummies'], columns=['Embarked_dummies', 'Title_dummies', 'Pclass', 'Pont_de_Cabin_dummies'])\n",
    "```\n",
    "\n",
    "Nous avons sélectionné de manière intuitive 4 colonnes qui pourraient potentiellement présenter une correlation et nous allons afficher les résultats obtenus au sein d'une matrice en utilisant la bibliothèque seaborn : \n",
    "\n",
    "```\n",
    "# Plot the covariance matrix using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cov_df, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Covariance Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Validation \n",
    "\n",
    "Nous allons procéder à la réalisation de notre validation en ajoutant nos nouvelles features pour entraîner notre modèle :\n",
    "\n",
    "```\n",
    "titanic_features = ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Sex', 'Age', 'Embarked_dummies', 'Title_dummies', 'Pont_de_Cabin_dummies']\n",
    "X = train_data[titanic_features]\n",
    "y = train_data.Survived\n",
    "```\n",
    "\n",
    "##### La méthode Hold-out\n",
    "\n",
    "```\n",
    "## Validation \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8) ## Possibilité de split le dataset différemment selon un paramètre à chercher pour les prochaines itérations\n",
    "\n",
    "# instanciation du modèle\n",
    "titanic_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# entraînement (fit)\n",
    "titanic_model.fit(X_train, y_train)\n",
    "\n",
    "# obtenir un score global (par défaut la metrique obtenu correspond à l'accuracy)\n",
    "titanic_model.score(X_test, y_test)\n",
    "\n",
    "## Le score obtenu est négatif, il devrait être compris entre 0 et 1. La méthode `.score` renvoie un coefficient de détermination R2 qui mesure la proportion de la variance des données qui est expliquée par le modèle. Un R2 de 1 indique un modèle parfait, tandis qu'un R2 de 0 indique que le modèle ne fait pas mieux que la moyenne des données.\n",
    "\n",
    "Comment traduire ce résulat ? -0.027125717266562432\n",
    "```\n",
    "\n",
    "Nous décidons de poursuivre jusqu'à la soumission de notre nouvelle itération personnel quand bien même nous ne savons expliquer ce score négatif. \n",
    "\n",
    "### Score : \n",
    "Nous obtenons un score de **0.69377**. En relançant notre jupyter nptebook tout en récoltant les résultats de notre modèle dans un nouveau fichier de soumission nous obtenons aussi **0.56459** et **0.64354**.\n",
    "\n",
    "### Conclusion : \n",
    "En encodant nos nouvelles colonnes nous avons choisis d'ajouter trois features à nos variables caractéristiques : `Title_dummies`, `Embarked_dummies` et `Pont_de_Cabin_dummies`. Nous perdons entre 0.15 et 0.5 points par rapport à notre précédente soumission. Il y a d'autres voies encore à explorer que nous allons vous partagez dans notre prochaine itération."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d92d3db-8884-4094-9f49-8d147ed5ee99",
   "metadata": {},
   "source": [
    "# Cinquième Itération"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c925e5-fbb3-4686-8211-de809bff258b",
   "metadata": {},
   "source": [
    "## Titre : Essai sur un autre modèle\n",
    "\n",
    "### Algorithme : RandomForest\n",
    "\n",
    "### Description : \n",
    "\n",
    "### Score : \n",
    "Nous avons obtenu un score de **0.77511** en prenant seulement 4 features (`Pclass`, `Sex`, `SibS`, `Parch`) encodées. \n",
    "\n",
    "### Conclusion : \n",
    "Bien que nous ayons utilisé que 4 features et sans faire un travail poussé en matière de nettoyage ou feature engineering nous obtenons un score supérieur à toutes les itérations préalablement réalisées. Le modèle random forest semble beaucoup plus approprié pour notre sujet. Dans les prochaines itérations, nous allons appliquer les mêmes stratégies d'imputation qu'appliquées lors des itérations réalisées avec le modèle de regression linéaire `DecisionTreeRegressor`. Nous espérons obtenir de meilleurs résultats encore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5aa9a-8b67-44c3-9847-caefdde9795a",
   "metadata": {},
   "source": [
    "# Sixième Itération"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334628c5-910e-4791-8dbb-a89100a1b9b7",
   "metadata": {},
   "source": [
    "## Titre : Essai sur un autre modèle (V2)\n",
    "\n",
    "### Algorithme : RandomForest\n",
    "\n",
    "### Description : \n",
    "\n",
    "### Score :\n",
    "Nous avons obtenu un score de **0.78708** en rajoutant les features `Age` et `Fare` \n",
    "\n",
    "### Conclusion :\n",
    "Bien que nous ayons utilisé que 6 features et en appliquant une méthode plus poussé pour remplacer les valeurs manquantes présentes dans nos colonnes `Age` et `Fare` nous gagnons 0.01 points. Ce n'est pas grand chose mais qui reste toujours une progression. Nous allons poursuivre sur ce Modèle Radom Forest en opérant du feature engineering. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
